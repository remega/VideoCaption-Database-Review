# Video caption database

**Based on yoosan's repostitory: https://github.com/yoosan/video-understanding-dataset**


## Video Captioning 
Dataset | Paper | Website | Context | #Examples | Organizer & Description | Average length(s) | Total length (h)
--------|-------|---------|----------|-----------|-----------|-----------------| ----------------|
MPII-MD |[PDF][v2]| [Link][c2] | movie | 68,337 clips with 68,375 sentences| MPII | 4.1 | 77.8
MSR-VTT |[PDF][v1]| [Link][c1] | 20 categories| 10,000 clips wth 200,000 sentences| MSR | - | -
Charades |[PDF][p4]| [Link][l4] | human activity| 9,848 clips wth 27,847 sentences| AI2 | - | -
Densevid |[PDF][v3]| [Link][c3] | event | 20k clips and 100k sentences | Stanford, ActivityNet | - | -
LSMDC |[PDF][v4]| [Link][c4] | movie | 128,085 clips and 128,118 sentences | Max Planck, MPII-MD + M-VAD | 4.1 | 147
VAS |[PDF][v4]| [Link][c4] | movie | 144 clips and ave 3 sentences | SNU, With eye-tracking data| 15 | -

[v1]: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf
[c1]: http://ms-multimedia-challenge.com/2017/
[v2]: https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf
[c2]: https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/mpii-movie-description-dataset/
[v3]: https://arxiv.org/abs/1705.00754
[c3]: https://cs.stanford.edu/people/ranjaykrishna/densevid/
[v4]: https://link.springer.com/article/10.1007/s11263-016-0987-1
[c4]: https://sites.google.com/site/describingmovies/
[p4]: https://link.springer.com/chapter/10.1007/978-3-319-46448-0_31
[l4]: http://allenai.org/plato/charades/

[v5]: http://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Supervising_Neural_Attention_CVPR_2017_paper.pdf
[c5]: 
